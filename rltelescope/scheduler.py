"""
Uses the weights as generated by the rl agent as coefficients to 
produce an observation for a ground telescope.
Configures each trained model and outputs both the weight producing model and the
"""
import math
import configparser
import os
import ast

import pandas as pd
import numpy as np

import astropy.units as u
from observation_program import ObservationProgram


class Scheduler:
    """
    Semiabstract scheduler class, collects the schedule itself 
    and passes over different actions to advance the observation program
    """
    def __init__(self, config, obsprog_config):
        """
        :param config: Config path for the schedule itself
        :param obsprog_config: Config path for the obversation program
        """
        assert os.path.exists(config)
        self.config = configparser.ConfigParser()
        self.config.read(config)
        duration = self.config.getfloat("schedule", "length")

        assert os.path.exists(obsprog_config)
        self.obsprog = ObservationProgram(obsprog_config, duration)

        self.actions = self.generate_action_table()
        self.invalid_reward = self.config.getfloat("reward", "invalid_reward")

        schedule_cols = [
            "mjd",
            "end_mjd",
            "ra",
            "decl",
            "band",
            "exposure_time",
            "reward",
        ]
        self.schedule = pd.DataFrame(columns=schedule_cols)

    def generate_action_table(self):
        # Based on the config params
        # Generate the number of allowed actions based on the
        # Parameters of each action allowance
        actions = pd.DataFrame(columns=["ra", "decl", "band"])

        min_ra = self.config.getfloat("actions", "min_ra")
        max_ra = self.config.getfloat("actions", "max_ra")
        n_ra = self.config.getint("actions", "num_ra_steps")
        ra_range = np.linspace(min_ra, max_ra, num=n_ra)

        min_decl = self.config.getfloat("actions", "min_decl")
        max_decl = self.config.getfloat("actions", "max_decl")
        n_decl = self.config.getint("actions", "num_decl_steps")
        decl_range = np.linspace(min_decl, max_decl, num=n_decl)

        bands = ast.literal_eval(self.config.get("actions", "bands"))

        ra_range = ra_range if len(ra_range) != 0 else [0]
        decl_range = decl_range if len(decl_range) != 0 else [0]
        bands = bands if len(bands) != 0 else ["g"]

        # Produce a row corresponding to each action, which is a combination of all possible
        # options for each part of the action
        for ra in ra_range:
            for decl in decl_range:
                for band in bands:
                    new_action = {"ra": ra, "decl": decl, "band": band}
                    new_action = pd.DataFrame(new_action, index=[len(actions)])
                    actions = pd.concat([actions, new_action])

        # Exposure time is constant
        actions["exposure_time"] = self.config.getfloat("actions", "exposure_time")

        # TODO Option for just a list of actions
        return actions

    def update(self, start_date, end_date):
        # Generate a schedule that sits between these two times
        raise NotImplementedError

    def feed_action(self, action):
        # Apply the action to the observation prog and update the state
        self.obsprog.update_observation(**action)
        new_observation = self.obsprog.state
        return new_observation

    def save(self, outpath):
        # Save the current schedule
        if not os.path.exists(outpath):
            os.makedirs(outpath)
        schedule_name = f"{outpath.rstrip('/')}/schedule.csv"
        self.schedule.to_csv(schedule_name)

    def reward(self, observation):
        # Get the reward for the given action
        if not self.invalid_action(observation):
            reward = self.invalid_reward
        else:
            reward = self.teff_reward(observation)
        return reward

    def teff_reward(self, observation):
        return (
            observation["teff"]
            if observation["teff"] is not None
            else self.invalid_reward
        )

    @staticmethod
    def to_rad(degrees):
        return math.radians(degrees)

    def calculate_action(self, **action_params):
        # Decide the next action. Implemented in children classes
        raise NotImplementedError

    def update_schedule(self, action, reward):
        # Add a new row to the schedule
        action["reward"] = reward
        new_action = pd.DataFrame(action, index=[len(self.schedule)])
        self.schedule = pd.concat([self.schedule, new_action])

    def check_endtime(self, action):
        # Apply an end condition that verifies that the schedule is equal to the length specified
        # in the config
        done = False
        length = self.config.getfloat("schedule", "length")
        end_time = self.obsprog.start_time + length / 24
        if action["end_mjd"] >= end_time:
            done = True
        return done
